{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manisha021095/Manisha_INFO5731_Fall2022/blob/main/In_class_exercise_02_09222022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdewF-EfgLW1"
      },
      "source": [
        "## The second In-class-exercise (09/22/2022, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OabuoqiugLW2"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4De4Y7kagLW3"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xt8NEe16gLW3",
        "outputId": "78e40269-0ed0-4d87-a438-38b2b2325de8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nResearch question - What is the review of Nokia Andriod smart TV. Whether it has positive reviews or negative reviews.\\nData required for this research is reviews of the customer who already bought the TV. Ecommerce website contains the reviews \\nof the product. I have choosen the flipkart to collect the reviews of the Nokia Andriod smart TV. We can analyze the reviews \\nby considering the words in them that is whether the user customer has written in a positive way or negative way. We need \\nminimum 500 reviews to get nearly accurate results.\\n\\nSteps for Collecting and Saving Data:\\nI have used the BeautifulSoup library to extract the information from the website.\\nI have extracted the reviews by using the classname and then appended to the empty list.\\nTo extract 500 reviews I have itearted 50 times as each page contains 10 reviews and I have generated the url dynamically while \\niterating.\\nThen created a dataframe form the list and then converted dataframe to csv\\n\\n\\n\\n\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "\n",
        "'''\n",
        "Research question - What is the review of Nokia Andriod smart TV. Whether it has positive reviews or negative reviews.\n",
        "Data required for this research is reviews of the customer who already bought the TV. Ecommerce website contains the reviews \n",
        "of the product. I have choosen the flipkart to collect the reviews of the Nokia Andriod smart TV. We can analyze the reviews \n",
        "by considering the words in them that is whether the user customer has written in a positive way or negative way. We need \n",
        "minimum 1000 reviews to get nearly accurate results.\n",
        "`\n",
        "Steps for Collecting and Saving Data:\n",
        "I have used the BeautifulSoup library to extract the information from the website.\n",
        "I have extracted the reviews by using the classname and then appended to the empty list.\n",
        "To extract 1000 reviews I have itearted 100 times as each page contains 10 reviews and I have generated the url dynamically while \n",
        "iterating.\n",
        "Then created a dataframe form the list and then converted dataframe to csv\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF3eOAmvgLW5"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "eNfmCHY4gLW5"
      },
      "outputs": [],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "main_text = [] # List to store Review headings\n",
        "sub_text =[] #List to store reviews\n",
        "\n",
        "for number in range(1,101):\n",
        "    link = \"https://www.flipkart.com/nokia-139cm-55-inch-ultra-hd-4k-led-smart-android-tv-sound-jbl/product-reviews/itmffvfvyztsmfmq?pid=TVSFFVFVJEGZ3R5H&lid=LSTTVSFFVFVJEGZ3R5HW9DJ5W&marketplace=FLIPKART&page=\" + str(number) # Generating link dynamically\n",
        "    page = requests.get(link) # Accessing the webpage\n",
        "    soup = BeautifulSoup(page.text, 'html.parser')\n",
        "    main_reviews = soup.find_all(class_='_2-N8zT') # Getting the Review Heading by using the class name\n",
        "    text_reviews = soup.find_all(class_='t-ZTKy') # Getting the full reviews by using the class name\n",
        "    for ele, sub_ele in zip(main_reviews, text_reviews) : # Iterating through the list\n",
        "        main_text.append(ele.text) #Appending to empty list\n",
        "        sub_text.append(sub_ele.text)\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdPi9B1xgLW5",
        "outputId": "e39275a2-1ccf-407b-9776-8dce803b6c1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of data frame is 1000\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Glimpse of Review</th>\n",
              "      <th>Full Review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Review from Technology Gyan: Almost everything...</td>\n",
              "      <td>This 55\" 4K Nokia TV at this price point comes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Brilliant</td>\n",
              "      <td>I must say it is best decision to by Nokia TV....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Best in the market!</td>\n",
              "      <td>Flipcart delivered the Product in less than 24...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pretty good</td>\n",
              "      <td>Pros1) Picture Quality is good, micro dimming ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Terrific purchase</td>\n",
              "      <td>Value for money ....1) This tv has Everything ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>Perfect product!</td>\n",
              "      <td>I have reviewed ofter using 30 days. Tv is rea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>Classy product ,Good</td>\n",
              "      <td>very good TV, Everything fine. Go for it.READ ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>Wonderful</td>\n",
              "      <td>The product does well for a budget TV.Good pic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>Not good</td>\n",
              "      <td>No thanksREAD MORE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>better</td>\n",
              "      <td>goodREAD MORE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     Glimpse of Review  \\\n",
              "0    Review from Technology Gyan: Almost everything...   \n",
              "1                                            Brilliant   \n",
              "2                                  Best in the market!   \n",
              "3                                          Pretty good   \n",
              "4                                    Terrific purchase   \n",
              "..                                                 ...   \n",
              "995                                   Perfect product!   \n",
              "996                               Classy product ,Good   \n",
              "997                                          Wonderful   \n",
              "998                                           Not good   \n",
              "999                                             better   \n",
              "\n",
              "                                           Full Review  \n",
              "0    This 55\" 4K Nokia TV at this price point comes...  \n",
              "1    I must say it is best decision to by Nokia TV....  \n",
              "2    Flipcart delivered the Product in less than 24...  \n",
              "3    Pros1) Picture Quality is good, micro dimming ...  \n",
              "4    Value for money ....1) This tv has Everything ...  \n",
              "..                                                 ...  \n",
              "995  I have reviewed ofter using 30 days. Tv is rea...  \n",
              "996  very good TV, Everything fine. Go for it.READ ...  \n",
              "997  The product does well for a budget TV.Good pic...  \n",
              "998                                 No thanksREAD MORE  \n",
              "999                                      goodREAD MORE  \n",
              "\n",
              "[1000 rows x 2 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.DataFrame(list(zip(main_text, sub_text)), columns =['Glimpse of Review', 'Full Review'])  # Creating Dataframe\n",
        "print(\"Length of data frame is {0}\".format(len(df)))\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-xX2zMngLW5"
      },
      "outputs": [],
      "source": [
        "df.to_csv('Question2_data.csv', index=False) #Converting to csv file from dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWMYdb7BgLW6"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2012-2022).\n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCFYDTdZgLW6"
      },
      "outputs": [],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "key_word = \"information retrieval\"\n",
        "\n",
        "\n",
        "key_word_format = \"+\".join(key_word.split(\" \"))\n",
        "\n",
        "\n",
        "check_link =\"https://dl.acm.org/action/doSearch?AllField=\" + key_word_format + \"&pageSize=50&startPage=\" \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsqLWIlkgLW7",
        "outputId": "af56a5b2-0d5b-4a30-aaa0-60310931a013"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page:  37\n",
            "page:  43\n"
          ]
        }
      ],
      "source": [
        "Req_df = pd.DataFrame(columns = [\"Title\", \"Venue/Journal/Conf\", \"Year\", \"Authors\", \"Abstract\"])\n",
        "\n",
        "for i in range(1,200):\n",
        "    try:\n",
        "        search_link = check_link + str(i)\n",
        "        page = requests.get(search_link)\n",
        "        soup = BeautifulSoup(page.text, 'html.parser')\n",
        "        titles = soup.find_all(class_ = \"hlFld-Title\")\n",
        "        for j in titles:\n",
        "            if len(Req_df) < 1001:       \n",
        "                sub_link = \"https://dl.acm.org\" + j.find(\"a\").get(\"href\")\n",
        "                sub_page = requests.get(sub_link)\n",
        "                sub_soup = BeautifulSoup(sub_page.text, 'html.parser')\n",
        "                year_j = sub_soup.find_all(class_ = \"CitationCoverDate\")[0].text.split(\" \")[-1]\n",
        "                if 2011 < int(year_j) < 2023:\n",
        "                    Req_df = Req_df.append({\n",
        "                        \"Title\"              : sub_soup.find_all(class_ = \"citation__title\")[0].text,\n",
        "                        \"Venue/Journal/Conf\" : sub_soup.find_all(class_ = \"issue-item__detail\")[0].find(\"a\").get(\"title\"),\n",
        "                        \"Year\"               : year_j,\n",
        "                        \"Authors\"            : sub_soup.find_all(class_ = \"author-data\")[0].text,\n",
        "                        \"Abstract\"           : sub_soup.find_all(class_ = \"hlFld-Abstract\")[0].text[8:].replace(\"\\n\", \"\")\n",
        "                    }, ignore_index = True)\n",
        "                #sub_page.close()\n",
        "            else:\n",
        "                break\n",
        "    except:\n",
        "        print(\"page: \", i)\n",
        "    #page.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBM5Db-cgLW7",
        "outputId": "98eb18b7-0a26-44d7-b3e3-3269b14e22ab"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Venue/Journal/Conf</th>\n",
              "      <th>Year</th>\n",
              "      <th>Authors</th>\n",
              "      <th>Abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Interactive Information Retrieval with Bandit ...</td>\n",
              "      <td>SIGIR '21: Proceedings of the 44th Internation...</td>\n",
              "      <td>2021</td>\n",
              "      <td>Huazheng Wang</td>\n",
              "      <td>Information retrieval (IR) in nature is a proc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Proactive Information Retrieval via Screen Sur...</td>\n",
              "      <td>SIGIR '17: Proceedings of the 40th Internation...</td>\n",
              "      <td>2017</td>\n",
              "      <td>Tung Vuong</td>\n",
              "      <td>We demonstrate proactive information retrieval...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Private Stateful Information Retrieval</td>\n",
              "      <td>CCS '18: Proceedings of the 2018 ACM SIGSAC Co...</td>\n",
              "      <td>2018</td>\n",
              "      <td>Sarvar Patel</td>\n",
              "      <td>Private information retrieval (PIR) is a funda...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Crowdsourcing for information retrieval</td>\n",
              "      <td>ACM SIGIR Forum</td>\n",
              "      <td>2012</td>\n",
              "      <td>Matthew Lease</td>\n",
              "      <td>The 2nd SIGIR Workshop on Crowdsourcing for In...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bandit Algorithms in Interactive Information R...</td>\n",
              "      <td>ICTIR '17: Proceedings of the ACM SIGIR Intern...</td>\n",
              "      <td>2017</td>\n",
              "      <td>Dorota Glowacka</td>\n",
              "      <td>The multi-armed bandit problem models an agent...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>523</th>\n",
              "      <td>Report on the SIGIR 2013 workshop on modeling ...</td>\n",
              "      <td>ACM SIGIR Forum</td>\n",
              "      <td>2013</td>\n",
              "      <td>Charles L.A. Clarke</td>\n",
              "      <td>The SIGIR 2013 Workshop on Modeling User Behav...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>524</th>\n",
              "      <td>Effective Information Retrieval Approach based...</td>\n",
              "      <td>ICARCSET '15: Proceedings of the 2015 Internat...</td>\n",
              "      <td>2015</td>\n",
              "      <td>I. Govindharaj</td>\n",
              "      <td>The Information technology is developing rapid...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>525</th>\n",
              "      <td>A user defined taxonomy of factors that divide...</td>\n",
              "      <td>IIiX '14: Proceedings of the 5th Information I...</td>\n",
              "      <td>2014</td>\n",
              "      <td>Chaoyu Ye</td>\n",
              "      <td>Although research is increasingly interested i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>526</th>\n",
              "      <td>Report on the fifth workshop on exploiting sem...</td>\n",
              "      <td>ACM SIGIR Forum</td>\n",
              "      <td>2012</td>\n",
              "      <td>Jaap Kamps</td>\n",
              "      <td>There is an increasing amount of structure on ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>527</th>\n",
              "      <td>Towards an Optimal Dialog Strategy for Informa...</td>\n",
              "      <td>IUI '18: 23rd International Conference on Inte...</td>\n",
              "      <td>2018</td>\n",
              "      <td>Yunfeng Zhang</td>\n",
              "      <td>The emerging paradigm of dialogue interfaces f...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>528 rows √ó 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 Title  \\\n",
              "0    Interactive Information Retrieval with Bandit ...   \n",
              "1    Proactive Information Retrieval via Screen Sur...   \n",
              "2               Private Stateful Information Retrieval   \n",
              "3              Crowdsourcing for information retrieval   \n",
              "4    Bandit Algorithms in Interactive Information R...   \n",
              "..                                                 ...   \n",
              "523  Report on the SIGIR 2013 workshop on modeling ...   \n",
              "524  Effective Information Retrieval Approach based...   \n",
              "525  A user defined taxonomy of factors that divide...   \n",
              "526  Report on the fifth workshop on exploiting sem...   \n",
              "527  Towards an Optimal Dialog Strategy for Informa...   \n",
              "\n",
              "                                    Venue/Journal/Conf  Year  \\\n",
              "0    SIGIR '21: Proceedings of the 44th Internation...  2021   \n",
              "1    SIGIR '17: Proceedings of the 40th Internation...  2017   \n",
              "2    CCS '18: Proceedings of the 2018 ACM SIGSAC Co...  2018   \n",
              "3                                      ACM SIGIR Forum  2012   \n",
              "4    ICTIR '17: Proceedings of the ACM SIGIR Intern...  2017   \n",
              "..                                                 ...   ...   \n",
              "523                                    ACM SIGIR Forum  2013   \n",
              "524  ICARCSET '15: Proceedings of the 2015 Internat...  2015   \n",
              "525  IIiX '14: Proceedings of the 5th Information I...  2014   \n",
              "526                                    ACM SIGIR Forum  2012   \n",
              "527  IUI '18: 23rd International Conference on Inte...  2018   \n",
              "\n",
              "                 Authors                                           Abstract  \n",
              "0          Huazheng Wang  Information retrieval (IR) in nature is a proc...  \n",
              "1             Tung Vuong  We demonstrate proactive information retrieval...  \n",
              "2           Sarvar Patel  Private information retrieval (PIR) is a funda...  \n",
              "3          Matthew Lease  The 2nd SIGIR Workshop on Crowdsourcing for In...  \n",
              "4        Dorota Glowacka  The multi-armed bandit problem models an agent...  \n",
              "..                   ...                                                ...  \n",
              "523  Charles L.A. Clarke  The SIGIR 2013 Workshop on Modeling User Behav...  \n",
              "524       I. Govindharaj  The Information technology is developing rapid...  \n",
              "525            Chaoyu Ye  Although research is increasingly interested i...  \n",
              "526           Jaap Kamps  There is an increasing amount of structure on ...  \n",
              "527        Yunfeng Zhang  The emerging paradigm of dialogue interfaces f...  \n",
              "\n",
              "[528 rows x 5 columns]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Req_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have selected \"ACM Digital Libraries\" website to retrieve articles. There are only 528 articles with \"information retrieval\" keyword from last 10 years."
      ],
      "metadata": {
        "id": "yYO60HuNgUOf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paunPzh8gLW7"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data. \n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pKHtaqGgLW7",
        "outputId": "f7d8c5b3-0585-4d83-8dd8-0d78bfb1d855"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: snscrape in c:\\users\\manis\\anaconda3\\lib\\site-packages (0.4.3.20220106)\n",
            "Requirement already satisfied: requests[socks] in c:\\users\\manis\\anaconda3\\lib\\site-packages (from snscrape) (2.25.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\manis\\anaconda3\\lib\\site-packages (from snscrape) (3.0.12)\n",
            "Requirement already satisfied: lxml in c:\\users\\manis\\anaconda3\\lib\\site-packages (from snscrape) (4.6.3)\n",
            "Requirement already satisfied: pytz in c:\\users\\manis\\anaconda3\\lib\\site-packages (from snscrape) (2021.1)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\manis\\anaconda3\\lib\\site-packages (from snscrape) (4.9.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\manis\\anaconda3\\lib\\site-packages (from beautifulsoup4->snscrape) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\manis\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\manis\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\manis\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\manis\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (1.26.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\manis\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "!pip install snscrape\n",
        "import snscrape.modules.twitter as sntwitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdlNct_fgLW8"
      },
      "outputs": [],
      "source": [
        "attributes_container = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNwLmRNYgLW8"
      },
      "outputs": [],
      "source": [
        "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('data since:2021-01-01 until:2022-09-20').get_items()):\n",
        "    if i>1000:\n",
        "        break\n",
        "    attributes_container.append([tweet.user.username, tweet.date, tweet.content])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWhL9dlggLW8",
        "outputId": "43d6c995-21e1-4f0a-ef25-2a82ed80a124"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User</th>\n",
              "      <th>Date Created</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tigerladytexas</td>\n",
              "      <td>2022-09-19 23:59:59+00:00</td>\n",
              "      <td>@RoxineKing @AZ_Brittney Where did they get th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>davesnyder</td>\n",
              "      <td>2022-09-19 23:59:59+00:00</td>\n",
              "      <td>@bernardjhuang Be able to build briefs as part...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ProdukPantau</td>\n",
              "      <td>2022-09-19 23:59:59+00:00</td>\n",
              "      <td>Coba cek ini, deh: \"UNEED Nylon Kabel Data Typ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MLB_DATA_</td>\n",
              "      <td>2022-09-19 23:59:59+00:00</td>\n",
              "      <td>Julio Teheran(ATL) Age.24\\n33ÁôªÊùø11Âãù8Êïó\\nÈò≤Âæ°Áéá4.04\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>cereja_stranger</td>\n",
              "      <td>2022-09-19 23:59:59+00:00</td>\n",
              "      <td>Todo dia uma foto aleat√≥ria at√© falarem a data...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>data_diario</td>\n",
              "      <td>2022-09-19 23:39:22+00:00</td>\n",
              "      <td>üí• @CFKArgentina, sobre la Causa Vialidad: \"Vam...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>curtisbuzzard</td>\n",
              "      <td>2022-09-19 23:39:21+00:00</td>\n",
              "      <td>Great session on data literacy at the Maneuver...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>bookstand171</td>\n",
              "      <td>2022-09-19 23:39:21+00:00</td>\n",
              "      <td>fetterman of pennsylvania and biden of delawar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>info_daratan</td>\n",
              "      <td>2022-09-19 23:39:20+00:00</td>\n",
              "      <td>Idrus memuji kinerja KPU khususnya terkait kew...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1000</th>\n",
              "      <td>homo10sapiens</td>\n",
              "      <td>2022-09-19 23:39:20+00:00</td>\n",
              "      <td>A pesquisa data Povo √© real! E mostra outra re...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1001 rows √ó 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 User              Date Created  \\\n",
              "0      tigerladytexas 2022-09-19 23:59:59+00:00   \n",
              "1          davesnyder 2022-09-19 23:59:59+00:00   \n",
              "2        ProdukPantau 2022-09-19 23:59:59+00:00   \n",
              "3           MLB_DATA_ 2022-09-19 23:59:59+00:00   \n",
              "4     cereja_stranger 2022-09-19 23:59:59+00:00   \n",
              "...               ...                       ...   \n",
              "996       data_diario 2022-09-19 23:39:22+00:00   \n",
              "997     curtisbuzzard 2022-09-19 23:39:21+00:00   \n",
              "998      bookstand171 2022-09-19 23:39:21+00:00   \n",
              "999      info_daratan 2022-09-19 23:39:20+00:00   \n",
              "1000    homo10sapiens 2022-09-19 23:39:20+00:00   \n",
              "\n",
              "                                                  Tweet  \n",
              "0     @RoxineKing @AZ_Brittney Where did they get th...  \n",
              "1     @bernardjhuang Be able to build briefs as part...  \n",
              "2     Coba cek ini, deh: \"UNEED Nylon Kabel Data Typ...  \n",
              "3     Julio Teheran(ATL) Age.24\\n33ÁôªÊùø11Âãù8Êïó\\nÈò≤Âæ°Áéá4.04\\...  \n",
              "4     Todo dia uma foto aleat√≥ria at√© falarem a data...  \n",
              "...                                                 ...  \n",
              "996   üí• @CFKArgentina, sobre la Causa Vialidad: \"Vam...  \n",
              "997   Great session on data literacy at the Maneuver...  \n",
              "998   fetterman of pennsylvania and biden of delawar...  \n",
              "999   Idrus memuji kinerja KPU khususnya terkait kew...  \n",
              "1000  A pesquisa data Povo √© real! E mostra outra re...  \n",
              "\n",
              "[1001 rows x 3 columns]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweets_df = pd.DataFrame(attributes_container, columns=[\"User\", \"Date Created\", \"Tweet\"])\n",
        "tweets_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MZtoK63gLW8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}