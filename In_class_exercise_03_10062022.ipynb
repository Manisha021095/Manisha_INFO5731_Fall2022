{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manisha021095/Manisha_INFO5731_Fall2022/blob/main/In_class_exercise_03_10062022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CenZtfzlz-ly"
      },
      "source": [
        "## The third In-class-exercise (10/06/2022, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbZjiBSpz-l0"
      },
      "source": [
        "The purpose of this exercise is to understand text representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEyYtvRHz-l0"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZT2tDYrz-l1",
        "outputId": "352c1a4d-5f45-404b-9507-6f3a29ad69c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nPlease write you answer here:\\n\\nParts of Speech extraction is an interesting text categorization or text mining activity that I have utilised.\\nIt's a process of assigning various labels, known as POS tags, to all of the words in a given input, defining \\nthe parts of speech of all words. I'm extracting features from IMDB movie reviews that show the parts of speech \\nfor each word, such as noun, pronoun, and verb, as well as the count of each tag. A noun, singular, denoted by \\nthe letter NN, is a portion of speech that denotes distinct people, places, or objects.The past tense of the verb \\nVBD denotes a noun's action in the past. Words in the same clause are connected by conjunctions. \\nThe relationship between the preceding noun and another word is represented by the preposition preceding noun.\\nThe number of adjectives is used to denote a grammatically linked attribute of a noun.\\n\\nA good example of a text classification task would be sentiment analysis of a given text document or piece of writing. The sentiment analysis can be about the author's sentiment about a topic and the relative strength of his sentiment.\\nThe following features types are considered in building the machine learning model:\\n1. Bag of words (preprocessed to remove spaces,special characters and single character words) based on a bag of words model. The more frequent the word, the better it impact it has on the sentiment. Rare and in-frequent words don't reflect the author's sentiment as they are too specific or random.\\n2. Word count (the number of words in the document) - is a reliable indicator of how strongly the author feels about the text he/she writes\\n3. Character count (the nuumber of characters in the document) - is good differentiator based on length analysis\\n4. Sentence count (the number of sentences in the document) - is good differentiator based on length analysis \\n5. Average Sentence Length (the average sentence length) - is good differentiator based on length analysis \\n\\n\\n\""
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "Parts of Speech extraction is an interesting text categorization or text mining activity that I have utilised.\n",
        "It's a process of assigning various labels, known as POS tags, to all of the words in a given input, defining \n",
        "the parts of speech of all words. I'm extracting features from IMDB movie reviews that show the parts of speech \n",
        "for each word, such as noun, pronoun, and verb, as well as the count of each tag. A noun, singular, denoted by \n",
        "the letter NN, is a portion of speech that denotes distinct people, places, or objects.The past tense of the verb \n",
        "VBD denotes a noun's action in the past. Words in the same clause are connected by conjunctions. \n",
        "The relationship between the preceding noun and another word is represented by the preposition preceding noun.\n",
        "The number of adjectives is used to denote a grammatically linked attribute of a noun.\n",
        "\n",
        "A good example of a text classification task would be sentiment analysis of a given text document or piece of writing. The sentiment analysis can be about the author's sentiment about a topic and the relative strength of his sentiment.\n",
        "The following features types are considered in building the machine learning model:\n",
        "1. Bag of words (preprocessed to remove spaces,special characters and single character words) based on a bag of words model. The more frequent the word, the better it impact it has on the sentiment. Rare and in-frequent words don't reflect the author's sentiment as they are too specific or random.\n",
        "2. Word count (the number of words in the document) - is a reliable indicator of how strongly the author feels about the text he/she writes\n",
        "3. Character count (the nuumber of characters in the document) - is good differentiator based on length analysis\n",
        "4. Sentence count (the number of sentences in the document) - is good differentiator based on length analysis \n",
        "5. Average Sentence Length (the average sentence length) - is good differentiator based on length analysis \n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExETRmA2z-l2"
      },
      "source": [
        "Question 2 (20 points): Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjT7qrVZz-l3",
        "outputId": "7971b099-b89d-4271-8474-70fea89e72e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\manis\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\manis\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                             features labels\n",
            "0                                                          0\n",
            "1   several year ordered number cooky altho never ...      1\n",
            "2                                                          2\n",
            "3                                               loved      3\n",
            "4                                                          4\n",
            "5                                                good      5\n",
            "6                                                          6\n",
            "7   buy every year christmas time treat everyone e...      7\n",
            "8                                                          8\n",
            "9   good typical container cooky consider special ...      9\n",
            "10                                                        10\n",
            "11  cooky good store bought price plain wrong save...     11\n",
            "12                                                        12\n",
            "13  rarely eat cooky got costco last christmas bel...     13\n",
            "14                                                        14\n",
            "15   good especially chocolate lover chocolate cookie     15\n",
            "16                                                        16\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "    word_count labels\n",
            "0            0      0\n",
            "1           54      1\n",
            "2            0      2\n",
            "3            1      3\n",
            "4            0      4\n",
            "5            1      5\n",
            "6            0      6\n",
            "7           36      7\n",
            "8            0      8\n",
            "9           14      9\n",
            "10           0     10\n",
            "11           9     11\n",
            "12           0     12\n",
            "13          38     13\n",
            "14           0     14\n",
            "15           6     15\n",
            "16           0     16\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "    char_count labels\n",
            "0            0      0\n",
            "1          310      1\n",
            "2            0      2\n",
            "3            5      3\n",
            "4            0      4\n",
            "5            4      5\n",
            "6            0      6\n",
            "7          216      7\n",
            "8            0      8\n",
            "9           75      9\n",
            "10           0     10\n",
            "11          44     11\n",
            "12           0     12\n",
            "13         210     13\n",
            "14           0     14\n",
            "15          43     15\n",
            "16           0     16\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "    sentence_count labels\n",
            "0                1      0\n",
            "1               25      1\n",
            "2                1      2\n",
            "3                1      3\n",
            "4                1      4\n",
            "5                1      5\n",
            "6                1      6\n",
            "7                7      7\n",
            "8                1      8\n",
            "9                4      9\n",
            "10               1     10\n",
            "11               3     11\n",
            "12               1     12\n",
            "13               3     13\n",
            "14               1     14\n",
            "15               2     15\n",
            "16               1     16\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "    average_sentence_length labels\n",
            "0                  0.000000      0\n",
            "1                  2.160000      1\n",
            "2                  0.000000      2\n",
            "3                  1.000000      3\n",
            "4                  0.000000      4\n",
            "5                  1.000000      5\n",
            "6                  0.000000      6\n",
            "7                  5.142857      7\n",
            "8                  0.000000      8\n",
            "9                  3.500000      9\n",
            "10                 0.000000     10\n",
            "11                 3.000000     11\n",
            "12                 0.000000     12\n",
            "13                12.666667     13\n",
            "14                 0.000000     14\n",
            "15                 3.000000     15\n",
            "16                 0.000000     16\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "from os import defpath\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "HEADERS = ({'User-Agent':\n",
        "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) \\\n",
        "            AppleWebKit/537.36 (KHTML, like Gecko) \\\n",
        "            Chrome/90.0.4430.212 Safari/537.36',\n",
        "            'Accept-Language': 'en-US, en;q=0.5'})\n",
        "labels = []\n",
        "word_cnt_list = []\n",
        "char_cnt_list = []\n",
        "sentence_cnt_list = []\n",
        "avg_sen_len_list = []\n",
        "\n",
        "\n",
        "\n",
        "def word_count(document):\n",
        "    return len(document)\n",
        "\n",
        "def char_count(document):\n",
        "    val = 0\n",
        "    for word in document:\n",
        "        val += len(word)\n",
        "    return val\n",
        "\n",
        "def sentence_count(str):\n",
        "    return len(str.split(\".\"))\n",
        "\n",
        "\n",
        "def get_customer_reviews(soup):\n",
        "    str = \"\"\n",
        "\n",
        "    scraped_review_list = soup.find_all(\"div\",{\"data-hook\":\"review-collapsed\"})\n",
        "  #print(scraped_review_list)\n",
        "  \n",
        "    for review in scraped_review_list:\n",
        "        str = str + review.get_text()\n",
        "  #print(str)\n",
        "    return str.split(\"\\n\")\n",
        "\n",
        "\n",
        "def get_documents(url):\n",
        "    raw_html =  requests.get(url, headers=HEADERS).text\n",
        "  #print(raw_html)\n",
        "    soup = BeautifulSoup(raw_html, 'lxml')\n",
        "    return get_customer_reviews(soup)\n",
        "\n",
        "\n",
        "def get_lemmatized_docs(X):\n",
        "    stemmer = WordNetLemmatizer()\n",
        "    eng_stop_word_list = stopwords.words(\"english\")\n",
        "    bag_of_words = []\n",
        "    for sen in range(0, len(X)):\n",
        "      # Remove all the special characters\n",
        "        document = re.sub(r'\\W', ' ', str(X[sen]))\n",
        "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "        document = re.sub(r'^b\\s+', '', document)\n",
        "        document = document.lower()\n",
        "      \n",
        "      # Splitting the cleaned document into a list of words, which will be subsequently added to a bag after more processing\n",
        "        document = document.split()\n",
        "        document = [stemmer.lemmatize(word) for word in document]\n",
        "        document = [word for word in document if word not in eng_stop_word_list]\n",
        "      \n",
        "      #Bag of words\n",
        "        bag_of_words.append(document)\n",
        "\n",
        "      # Labels\n",
        "        labels.append(str(sen))\n",
        "\n",
        "      # Word count\n",
        "        word_cnt_list.append(word_count(document))\n",
        "\n",
        "      # Character count\n",
        "        char_cnt_list.append(char_count(document))\n",
        "\n",
        "      # Sentence count\n",
        "        sentence_cnt_list.append(sentence_count(str(X[sen])))\n",
        "\n",
        "      # Average Sentence Length\n",
        "        if(sentence_cnt_list[-1] == 0):\n",
        "            avg_sen_len_list.append(0)\n",
        "        else:\n",
        "            avg_sen_len_list.append(word_cnt_list[-1]/sentence_cnt_list[-1])\n",
        "      \n",
        "      # bag_of_words contains all the useful features or terms from the original document\n",
        "    return bag_of_words\n",
        "\n",
        "\n",
        "documents = get_documents(\"https://www.amazon.com/Kirkland-Signature-European-Cookies-Chocolate/dp/B003ZIR8YU?pd_rd_w=fQvtk&pf_rd_p=cf1a5a84-8a64-4d2b-b13a-eedb7e66da7d&pf_rd_r=VY2H4Y5276PSSE8782H8&pd_rd_r=6619c159-1009-439a-aadf-3a0aba1eb5a2&pd_rd_wg=vgrNq&pd_rd_i=B003ZIR8YU&ref_=pd_bap_d_rp_1_i&th=1\")\n",
        "cleaned_documents = get_lemmatized_docs(documents)\n",
        "#print(cleaned_documents)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame()\n",
        "df[\"features\"] = cleaned_documents\n",
        "df[\"features\"] = df[\"features\"].apply(lambda x: \" \".join(x))\n",
        "df[\"labels\"] = labels\n",
        "df[\"word_count\"] = word_cnt_list\n",
        "df[\"char_count\"] = char_cnt_list\n",
        "df[\"sentence_count\"] = sentence_cnt_list\n",
        "df[\"average_sentence_length\"] = avg_sen_len_list;\n",
        "#print(df)\n",
        "\n",
        "\n",
        "df1 = pd.DataFrame()\n",
        "df1['features'] = df[\"features\"]\n",
        "df1[\"labels\"] = labels\n",
        "print(df1)\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "df2 = pd.DataFrame()\n",
        "df2[\"word_count\"] =df[\"word_count\"]\n",
        "df2[\"labels\"] = labels\n",
        "print(df2)\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(\"\")\n",
        "\n",
        "df3 = pd.DataFrame()\n",
        "df3[\"char_count\"] =df[\"char_count\"]\n",
        "df3[\"labels\"] = labels\n",
        "print(df3)\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(\"\")\n",
        "\n",
        "df4 = pd.DataFrame()\n",
        "df4[\"sentence_count\"] =df[\"sentence_count\"]\n",
        "df4[\"labels\"] = labels\n",
        "print(df4)\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(\"\")\n",
        "\n",
        "df5 = pd.DataFrame()\n",
        "df5[\"average_sentence_length\"] =df[\"average_sentence_length\"]\n",
        "df5[\"labels\"] = labels\n",
        "print(df5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNSp4Hfhz-l4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh0GbtvHz-l4"
      },
      "source": [
        "Question 3 (10 points): Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\" Select the most important features you extracted above, rank the features based on their importance in the descending order. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCw2-lV-z-l5",
        "outputId": "7a6db39b-9d94-4300-834c-655fbc9f095f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         term  cumulative_TF-IDF\n",
            "42       good           1.980550\n",
            "24      cooky           1.176829\n",
            "62      loved           1.000000\n",
            "17  chocolate           0.839821\n",
            "79      price           0.472542\n",
            "..        ...                ...\n",
            "74    ordered           0.140754\n",
            "76        pkg           0.140754\n",
            "80    problem           0.140754\n",
            "82  recipient           0.140754\n",
            "57      label           0.140754\n",
            "\n",
            "[114 rows x 2 columns]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\manis\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "# Utilizing the TF-IDF filter model mention \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidfconverter = TfidfVectorizer(analyzer='word', stop_words = 'english')\n",
        "X = tfidfconverter.fit_transform(df[\"features\"])\n",
        "\n",
        "terms = tfidfconverter.get_feature_names()\n",
        "\n",
        "# sum tfidf frequency of each term through documents\n",
        "sums = X.sum(axis=0)\n",
        "\n",
        "data = []\n",
        "for col, term in enumerate(terms):\n",
        "    data.append( (term, sums[0,col] ))\n",
        "\n",
        "sorted_features = pd.DataFrame(data, columns=['term','cumulative_TF-IDF'])\n",
        "print(sorted_features.sort_values('cumulative_TF-IDF', ascending=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZktEFyHz-l5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}